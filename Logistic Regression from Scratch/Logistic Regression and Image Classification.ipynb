{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1 \n",
    "\n",
    "\n",
    "Logistic Regression is easy to implement but still has great training \n",
    "efficiency and is one of the simplest machine learning algorithms. It is also easy to train model with \n",
    "this algorithm because it doesnâ€™t require high computation power and the training time of this \n",
    "algorithm is also far less than other complex algorithms. One of the biggest advantage over other \n",
    "models which only gives the final classification as results is that logistics regression also outputs \n",
    "well-calibrated probabilities along with classification results and we can also update model to easily \n",
    "reflect new data, unlike decision tree. So, if a training example gives us 90% probability for a class, \n",
    "and another training example gives us 70% probability for same class, we can inference about which\n",
    "training examples are more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions used : Done by Anshul\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function :\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "#Hyperbolic Arctan function\n",
    "def arctan(x):\n",
    "    return np.arctan(x)\n",
    "\n",
    "#Relu\n",
    "def relu(Z):  \n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization with backpropagation : Done by Kashika\n",
    "def optimize(x, y,learning_rate,iterations,weight,bias): \n",
    "    size = x.shape[0]\n",
    "    for i in range(iterations): \n",
    "        output = sigmoid(np.dot(x, weight) + bias)\n",
    "        loss = -1/size * np.sum(y * np.log(output)) + (1 - y) * np.log(1-output)\n",
    "        dW = 1/size * np.dot(x.T, (output - y))\n",
    "        db = 1/size * np.sum(output - y)\n",
    "        weight -= learning_rate * dW\n",
    "        bias -= learning_rate * db \n",
    "    return weight,bias\n",
    "\n",
    "# Function to train and optimize the model\n",
    "def train(x, y, learning_rate,iterations):\n",
    "    weight_new,bias_new = optimize(x, y, learning_rate, iterations ,weight,bias)\n",
    "    return weight_new,bias_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used the code created in Machine Learning Assignment 2 \n",
    "\n",
    "# Find the min and max values for each column\n",
    "def minmax_finder(data):\n",
    "    minmax = list()\n",
    "    for i in range(len(x.columns)):\n",
    "            col_vals = [row[i] for row in data]\n",
    "            min_val = min(col_vals)\n",
    "            max_val = max(col_vals)\n",
    "            minmax.append([min_val, max_val])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_data(data, minmax):\n",
    "    for row in data:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "\n",
    "After running the above logistic regression model on both of these datasets the accuracy for 'blobs250' is 100% in both validation and test dataset as it is a linear dataset and for 'moons400' is 90% and 81% respectively as it is a non linear dataset.\n",
    "\n",
    "Dataset 1 : blobs250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.961400</td>\n",
       "      <td>5.677191</td>\n",
       "      <td>11.407020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.372228</td>\n",
       "      <td>5.335292</td>\n",
       "      <td>9.460564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.022249</td>\n",
       "      <td>7.501127</td>\n",
       "      <td>9.072816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.464773</td>\n",
       "      <td>7.819388</td>\n",
       "      <td>9.183951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.191087</td>\n",
       "      <td>5.880269</td>\n",
       "      <td>10.119531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X0        X1         X2  Class\n",
       "0  0.961400  5.677191  11.407020      0\n",
       "1  2.372228  5.335292   9.460564      0\n",
       "2  2.022249  7.501127   9.072816      0\n",
       "3  4.464773  7.819388   9.183951      0\n",
       "4  1.191087  5.880269  10.119531      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"blobs250.csv\")\n",
    "#separating target features from other features \n",
    "y = df['Class'].values\n",
    "x = df.drop('Class', axis =1)\n",
    "x_1 = x.values.astype(float)\n",
    "#  Normalize columns\n",
    "#minmax = minmax_finder(x_1)\n",
    "#normalize_data(x_1, minmax)\n",
    "#x = pd.DataFrame(x_1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using traintestsplit to split the data into training, validation and trsting\n",
    "x_dum, X_test, y_dum, y_test = train_test_split(x,y,test_size=0.15,train_size=0.85)\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_dum,y_dum,test_size = 0.15,train_size =0.85)\n",
    "\n",
    "#Initialize weights and bias\n",
    "weight = np.zeros(x.shape[1])\n",
    "bias = 0\n",
    "#call train function\n",
    "weight,bias = train(X_train, y_train, learning_rate = 0.02, iterations = 1000)\n",
    "\n",
    "# Predict the validation dataset using the trained model\n",
    "output_values = np.dot(X_val, weight) + bias\n",
    "predictions = sigmoid(output_values) >= 1/2\n",
    "accuracy_score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test dataset using the trained model\n",
    "output_values = np.dot(X_test, weight) + bias\n",
    "predictions = sigmoid(output_values) >= 1/2\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2 : moons400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"moons400.csv\")\n",
    "#separating target features from other features \n",
    "y = df['Class'].values\n",
    "x = df.drop('Class', axis =1)\n",
    "x_1 = x.values.astype(float) # returns a numpy array of type float\n",
    "#  Normalize columns\n",
    "minmax = minmax_finder(x_1)\n",
    "normalize_data(x_1, minmax)\n",
    "x = pd.DataFrame(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9019607843137255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dum, X_test, y_dum, y_test = train_test_split(x,y,test_size=0.15,train_size=0.85)\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_dum,y_dum,test_size = 0.15,train_size =0.85)\n",
    "\n",
    "#Initialize weights and bias\n",
    "weight = np.zeros(x.shape[1])\n",
    "bias = 0\n",
    "\n",
    "weight,bias = train(X_train, y_train, learning_rate = 0.02, iterations = 1000)\n",
    "\n",
    "# Predict the validation dataset using the trained model\n",
    "output_values = np.dot(X_val, weight) + bias\n",
    "predictions = sigmoid(output_values) >= 1/2\n",
    "accuracy_score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test dataset using the trained model\n",
    "output_values = np.dot(X_test, weight) + bias\n",
    "predictions = sigmoid(output_values) >= 1/2\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Shallow Neural Network\n",
    "\n",
    "The below shallow NN has 1 hidden layer(sigmoid function) and output layer(Hyperbolic tangent function).\n",
    "\n",
    "\n",
    "##### Results:\n",
    "Blobs250 : 53% Accuracy\n",
    "\n",
    "Moons400 : 51% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate and update the weights with 1 Hidden layer : Done by Anshul\n",
    "def NN_train(x,y,weight_layer1,weight_output,bias_layer1,bias_out,lr,iterations):\n",
    "    for epoch in range(iterations):\n",
    "\n",
    "     # Input and Output for Layer1\n",
    "        input_layer1 = np.dot(x, weight_layer1)\n",
    "        output_layer1 = sigmoid(input_layer1)\n",
    "     # Input and output for output layer :\n",
    "        input_output = np.dot(output_layer1, weight_output)\n",
    "        output_output = arctan(input_output)\n",
    "\n",
    "     # Calculate the Mean Squared Error :\n",
    "        MSE = ((1 / 2) * (np.power((output_output - y), 2)))\n",
    "\n",
    "     # Derivatives\n",
    "        error_ou = output_output - y   \n",
    "        der_1 = sigmoid_der(input_output)\n",
    "        ino_wo = output_layer1\n",
    "        error_out = np.dot(ino_wo.T, error_ou * der_1)\n",
    "        error_ino = error_ou * der_1\n",
    "        wt_out = weight_output\n",
    "        error_outh = np.dot(error_ino , wt_out)\n",
    "        outh_inh = sigmoid_der(input_layer1) \n",
    "        wt_in = x\n",
    "        error_in = np.dot(wt_in.T, outh_inh * error_outh)\n",
    "    # Update the weights\n",
    "        weight_layer1 = weight_layer1 - lr * error_in\n",
    "        weight_output -= lr * error_out \n",
    "    #Update the biases    \n",
    "        z_delta = error_ou*der_1\n",
    "        for num in z_delta:\n",
    "            bias_out -= lr*num\n",
    "        z_delta = error_ou*outh_inh\n",
    "        for num in z_delta:\n",
    "            bias_layer1 -= lr*num\n",
    "\n",
    "    return weight_layer1,weight_output,bias_layer1,bias_out\n",
    "\n",
    "#Function to predict the values \n",
    "def predictions(X,weight_hidden,weight_output,bias_layer1,bias_out,cutoff):\n",
    "    # Predictions :\n",
    "    part1 = sigmoid(np.dot(X, weight_hidden)+bias_layer1)\n",
    "    part2 = arctan(np.dot(part1,weight_output) + bias_out)  >= cutoff\n",
    "    return part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset : Blobs250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"blobs250.csv\")\n",
    "y = df['Class'].values\n",
    "x = df.drop('Class', axis =1)\n",
    "x_1 = x.values.astype(float)\n",
    "minmax = minmax_finder(x_1)\n",
    "normalize_data(x_1, minmax)\n",
    "x = pd.DataFrame(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=101)\n",
    "#input_features = X_train\n",
    "# Define target output :\n",
    "target_output = y_train\n",
    "y_train = y_train.reshape(len(target_output),1)\n",
    "weight_layer1 = np.random.rand(X_train.shape[1],1)\n",
    "weight_output = 0\n",
    "bias_out =0\n",
    "bias_layer1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5301204819277109"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden,weight_output,bias_layer1,bias_out = NN_train(X_train,y_train,weight_layer1,weight_output,bias_layer1,bias_out,lr=0.01,iterations=500)\n",
    "prediction = predictions(X_test,weight_layer1,weight_output,bias_layer1,bias_out,1/2)\n",
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset : Moons400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"moons400.csv\")\n",
    "y = df['Class'].values\n",
    "x = df.drop('Class', axis =1)\n",
    "x_1 = x.values.astype(float) # returns a numpy array of type float\n",
    "minmax = minmax_finder(x_1)\n",
    "normalize_data(x_1, minmax)\n",
    "x = pd.DataFrame(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=101)\n",
    "# Define target output :\n",
    "target_output = y_train\n",
    "y_train = y_train.reshape(len(target_output),1)\n",
    "weight_layer1 = np.random.rand(X_train.shape[1],1)\n",
    "weight_output = 0\n",
    "bias_out =0\n",
    "bias_layer1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5151515151515151"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden,weight_output,bias_layer1,bias_out = NN_train(X_train,y_train,weight_layer1,weight_output,bias_layer1,bias_out,lr=0.01,iterations=5000)\n",
    "prediction = predictions(X_test,weight_layer1,weight_output,bias_layer1,bias_out,1/2)\n",
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Image Recognition\n",
    "\n",
    "##### Classes : Automobile and Deers\n",
    "\n",
    "The model is trained with 0.01 learning rate and 500 epochs.\n",
    "##### Results: \n",
    "Validation Accuracy :52%\n",
    "\n",
    "Testing Accuracy    :49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the batch is 4\n",
      "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "size of data in this batch: 10000 , size of labels: 10000\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#The below code to load the dataset was already provided : Code already provided\n",
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "data = batch1[b'data']\n",
    "labels = batch1[b'labels']\n",
    "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "print (type(data))\n",
    "print(data.shape)\n",
    "\n",
    "names = loadlabelnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the classes 1- Automobile, 4-Deer : Done by Kashika\n",
    "index = []\n",
    "for i,label in enumerate(labels):\n",
    "    if label==1 or label ==4:\n",
    "        index.append(i)\n",
    "data_train = data[index]\n",
    "label_train =np.array(labels)\n",
    "label_final = label_train[index]\n",
    "\n",
    "#Selecting only 1 color\n",
    "X = np.array(data_train[:,:1024])\n",
    "from sklearn import preprocessing\n",
    "X_norm = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_norm, label_final, test_size=0.33, random_state=101) : Done by Kashika\n",
    "x_dum, X_test, y_dum, y_test = train_test_split(X_norm,label_final,test_size=0.15,train_size=0.85)\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_dum,y_dum,test_size = 0.15,train_size =0.85)\n",
    "\n",
    "input_features = X_train\n",
    "# Define target output :\n",
    "target_output = y_train\n",
    "# Reshaping our target output into vector :\n",
    "target_output = target_output.reshape(len(target_output),1)\n",
    "weight_hidden = np.random.rand(X_train.shape[1],1)\n",
    "weight_output = 0\n",
    "bias_layer1 = 0\n",
    "bias_out = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.26939114],\n",
       "        [0.02918426],\n",
       "        [0.90231431],\n",
       "        ...,\n",
       "        [0.93140208],\n",
       "        [0.09968117],\n",
       "        [0.86308507]]),\n",
       " array([[9.60621588]]),\n",
       " array([0.01164695]),\n",
       " array([9.60623341]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden,weight_output,bias_layer1,bias_out = NN_train(X_train,target_output,weight_hidden,weight_output,bias_layer1,bias_out,lr=0.01,iterations=500)\n",
    "weight_hidden,weight_output,bias_layer1,bias_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5198412698412699"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation\n",
    "prediction = predictions(X_val,weight_hidden,weight_output,bias_layer1,bias_out, 1/2)  \n",
    "accuracy_score(prediction, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4966216216216216"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "prediction = predictions(X_test,weight_hidden,weight_output,bias_layer1,bias_out, 1/2)  \n",
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task5 - Enhancement 1 - Change the Activation function in Neural Network\n",
    "\n",
    "##### Results: \n",
    "Validation Accuracy :52%\n",
    "\n",
    "Testing Accuracy    :49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train_En1(x,y,weight_layer1,weight_output,bias_layer1,bias_out,lr,iterations):\n",
    "    \n",
    "    for epoch in range(iterations):\n",
    "\n",
    "     # Input and Output for Layer1\n",
    "        input_layer1 = np.dot(x, weight_layer1)\n",
    "        output_layer1 = sigmoid(input_layer1)\n",
    "     # Input and output for output layer :\n",
    "        input_output = np.dot(output_layer1, weight_output)\n",
    "        output_output = relu(input_output)\n",
    "     # Calculate the Mean Squared Error :\n",
    "        MSE = ((1 / 2) * (np.power((output_output - y), 2)))\n",
    "\n",
    "     # Derivatives\n",
    "        error_ou = output_output - y   \n",
    "        der_1 = sigmoid_der(input_output)\n",
    "        ino_wo = output_layer1\n",
    "        error_out = np.dot(ino_wo.T, error_ou * der_1)\n",
    "        error_ino = error_ou * der_1\n",
    "        wt_out = weight_output\n",
    "        error_outh = np.dot(error_ino , wt_out)\n",
    "        outh_inh = sigmoid_der(input_layer1) \n",
    "        wt_in = x\n",
    "        error_in = np.dot(wt_in.T, outh_inh * error_outh)\n",
    "    # Update the weights\n",
    "        weight_layer1 = weight_layer1 - lr * error_in\n",
    "        weight_output -= lr * error_out \n",
    "    #Update the biases    \n",
    "        z_delta = error_ou*der_1\n",
    "        for num in z_delta:\n",
    "            bias_out -= lr*num\n",
    "        z_delta = error_ou*outh_inh\n",
    "        for num in z_delta:\n",
    "            bias_layer1 -= lr*num\n",
    "\n",
    "    return weight_layer1,weight_output,bias_layer1,bias_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_En1(X_test,weight_hidden,weight_output,bias_layer1,bias_out,cut_off):\n",
    "    # Predictions :\n",
    "    part1 = sigmoid(np.dot(X_test, weight_hidden)+bias_layer1)\n",
    "    part2 = relu(np.dot(part1,weight_output)+bias_out) >= cut_off\n",
    "    return part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_hidden = np.random.rand(X_train.shape[1],1)\n",
    "weight_output = 0\n",
    "bias_layer1 = 0\n",
    "bias_out = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.36242696],\n",
       "        [0.17065629],\n",
       "        [0.24237598],\n",
       "        ...,\n",
       "        [0.68994841],\n",
       "        [0.58921243],\n",
       "        [0.90836143]]),\n",
       " array([[2.53473968]]),\n",
       " array([-0.01596263]),\n",
       " array([2.53451942]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_hidden,weight_output,bias_layer1,bias_out = NN_train_En1(X_train,target_output,weight_hidden,weight_output,\n",
    "                                                                bias_layer1,bias_out,lr=0.01,iterations=500)\n",
    "weight_hidden,weight_output,bias_layer1,bias_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5198412698412699"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation\n",
    "prediction = predictions_En1(X_val,weight_hidden,weight_output,bias_layer1,bias_out, 1/2)  \n",
    "accuracy_score(prediction, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4966216216216216"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "prediction = predictions_En1(X_test,weight_hidden,weight_output,bias_layer1,bias_out, 1/2) \n",
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task5 Enhancement 2 - L2 Regularization\n",
    "\n",
    "L2 Regularization is used to reduce the overfitting by decreasing the value of weights. Although there is no change in the accuracy of the model it will still remain the same for this dataset. Although the weights and biases are different.\n",
    "\n",
    "##### Results: \n",
    "Validation Accuracy :52%\n",
    "\n",
    "Testing Accuracy    :49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train_En2(x,y,weight_layer1,weight_output,bias_layer1,bias_out,lr,iterations):\n",
    "    \n",
    "    for epoch in range(iterations):\n",
    "\n",
    "     # Input and Output for Layer1\n",
    "        input_layer1 = np.dot(x, weight_layer1)\n",
    "        output_layer1 = sigmoid(input_layer1)\n",
    "     # Input and output for output layer :\n",
    "        input_output = np.dot(output_layer1, weight_output)\n",
    "        output_output = arctan(input_output)\n",
    "     # Calculate the Mean Squared Error :\n",
    "        MSE = ((1 / 2) * (np.power((output_output - y), 2)))\n",
    "\n",
    "     # Derivatives\n",
    "        error_ou = output_output - y   \n",
    "        der_1 = sigmoid_der(input_output)\n",
    "        ino_wo = output_layer1\n",
    "        error_out = np.dot(ino_wo.T, error_ou * der_1)\n",
    "        error_ino = error_ou * der_1\n",
    "        wt_out = weight_output\n",
    "        error_outh = np.dot(error_ino , wt_out)\n",
    "        outh_inh = sigmoid_der(input_layer1) \n",
    "        wt_in = x\n",
    "        error_in = np.dot(wt_in.T, outh_inh * error_outh)\n",
    "        l2_lambda = 0.7\n",
    "        m = x.shape[0]\n",
    "    # Update the weights\n",
    "        weight_layer1 = weight_layer1 - lr * (error_in + ((l2_lambda/m) * weight_layer1))\n",
    "        weight_output -= lr * error_out \n",
    "    #Update the biases    \n",
    "        z_delta = error_ou*der_1\n",
    "        for num in z_delta:\n",
    "            bias_out -= lr*num\n",
    "        z_delta = error_ou*outh_inh\n",
    "        for num in z_delta:\n",
    "            bias_layer1 -= lr*num\n",
    "    return weight_layer1,weight_output,bias_layer1,bias_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_En2(X_test,weight_hidden,weight_output,bias_layer1,bias_out,cut_off):\n",
    "    # Predictions :\n",
    "    part1 = sigmoid(np.dot(X_test, weight_hidden)+bias_layer1)\n",
    "    part2 = arctan(np.dot(part1,weight_output)+bias_out) >= cut_off\n",
    "    return part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_hidden = np.random.rand(X_train.shape[1],1)\n",
    "weight_output = 0\n",
    "bias_layer1 = 0\n",
    "bias_out = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_hidden,weight_output,bias_layer1,bias_out = NN_train_En2(input_features,target_output,weight_hidden,bias_layer1,bias_out,weight_output,lr=0.01,iterations=500\n",
    "                                            )\n",
    "weight_hidden,weight_output,bias_layer1,bias_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5198412698412699"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation\n",
    "prediction = predictions_En2(X_val,weight_hidden,weight_output,bias_layer1,bias_out, 1/2)  \n",
    "accuracy_score(prediction, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4966216216216216"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "prediction = predictions_En2(X_test,weight_hidden,weight_output,bias_layer1,bias_out, 1/2) \n",
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References :\n",
    "\n",
    "[1] https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu\n",
    "\n",
    "[2] https://towardsdatascience.com/building-a-neural-network-with-a-single-hidden-layer-using-numpy-923be1180dbf\n",
    "\n",
    "[3] https://visualstudiomagazine.com/articles/2017/09/01/neural-network-l2.aspx\n",
    "\n",
    "[4] Week3 - Deep learning Lecture slides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
